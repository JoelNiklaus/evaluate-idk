# evaluate-idk
Evaluates to what extent LLMs signal correctly that they don't know the answer to a question

## Overview âœ¨
This repo evaluates how reliably LLMs say "I don't know" when they should, and how that calibration affects end-to-end utility. ğŸ¤–â“ It computes traditional accuracy, an IDK-aware score, the frequency of abstentions, and any extraction failures during parsing. The included runner evaluates models on the challenging GPQA Diamond and LEXam English benchmarks and aggregates results for quick comparison. ğŸ§ªğŸ“Š

So far, this repo supports GPQA-Diamond and LEXam-English.

## Results ğŸ“Š

### GPQA-Diamond
Latest aggregated results (as of September 26, 2025):

| Model                  | trad_score Â± se | idk_score Â± se | idk_freq Â± se | extract_fail Â± se |
| :--------------------- | --------------: | -------------: | ------------: | ----------------: |
| gemini-2.5-pro         |    83.84 Â± 2.62 |   67.68 Â± 5.25 |   0.00 Â± 0.00 |       2.02 Â± 1.00 |
| gpt-5                  |    82.83 Â± 2.69 |   68.69 Â± 5.03 |   3.03 Â± 1.22 |       3.03 Â± 1.22 |
| gpt-5-mini             |    79.29 Â± 2.89 |   60.10 Â± 5.63 |   1.52 Â± 0.87 |       1.01 Â± 0.71 |
| deepseek-v3.1-terminus |    71.21 Â± 3.23 |   47.47 Â± 6.06 |   5.05 Â± 1.56 |       0.00 Â± 0.00 |
| minimax-m2:free        |    68.18 Â± 3.32 |   39.39 Â± 6.43 |   3.03 Â± 1.22 |       3.54 Â± 1.32 |
| claude-sonnet-4        |    67.68 Â± 3.33 |   41.41 Â± 6.24 |   6.06 Â± 1.70 |       0.00 Â± 0.00 |
| gemini-2.5-flash       |    64.65 Â± 3.41 |   32.83 Â± 6.60 |   3.54 Â± 1.32 |       5.56 Â± 1.63 |
| gpt-5-nano             |    64.65 Â± 3.41 |   39.39 Â± 6.14 |  10.10 Â± 2.15 |       0.00 Â± 0.00 |
| gpt-4.1                |    63.13 Â± 3.44 |   27.78 Â± 6.79 |   1.52 Â± 0.87 |       0.00 Â± 0.00 |
| gpt-4.1-mini           |    61.62 Â± 3.46 |   27.27 Â± 6.70 |   4.04 Â± 1.40 |       0.00 Â± 0.00 |

![Performance drop visualization](results/figures/score_drop_barchart_gpqa.png)

#### Quick analysis ğŸ”
IDK-aware performance broadly mirrors traditional accuracy, with one notable swap at the top: GPTâ€‘5 edges Gemini 2.5 Pro on idk_score even though Pro leads trad_score and Pro never abstains (idk_freq â‰ˆ 0). DeepSeek v3.1 and Claude Sonnet 4 use the E option relatively often, narrowing their tradâ†’idk gap but still trailing the leaders; GPTâ€‘4.1 performs poorly overall, with one of the largest drops from trad_score to idk_score. GPTâ€‘5 mini is a standout for its size, combining strong accuracy with solid idk_score. Apart from the GPTâ€‘5 vs Gemini Pro reversal, ordering by idk_score largely matches trad_score, though the size of the tradâ†’idk gap varies meaningfully across models. Smaller models tend to select E more (notably GPTâ€‘5 nano), which lifts idk_score enough to beat Gemini 2.5 Flash despite the same trad_score. Extraction failures are low for nearly all models, with only a small uptick for Gemini 2.5 Flash.

### LEXam-English
Latest aggregated results (as of October 9, 2025):

| Model                  | trad_score Â± se | idk_score Â± se | idk_freq Â± se | extract_fail Â± se |
| :--------------------- | --------------: | -------------: | ------------: | ----------------: |
| gpt-5                  |    69.47 Â± 1.85 |   47.17 Â± 3.35 |   8.24 Â± 1.11 |       0.81 Â± 0.36 |
| gemini-2.5-pro         |    66.72 Â± 1.90 |   33.60 Â± 3.79 |   0.16 Â± 0.16 |       0.32 Â± 0.23 |
| gpt-5-mini             |    66.56 Â± 1.90 |   39.90 Â± 3.54 |   6.79 Â± 1.01 |       0.00 Â± 0.00 |
| gemini-2.5-flash       |    66.24 Â± 1.90 |   33.28 Â± 3.78 |   0.81 Â± 0.36 |       0.81 Â± 0.36 |
| claude-sonnet-4.5      |    64.62 Â± 1.92 |   35.70 Â± 3.62 |   6.46 Â± 0.99 |       0.32 Â± 0.23 |
| grok-4-fast            |    61.23 Â± 1.96 |   35.22 Â± 3.48 |  12.76 Â± 1.34 |       0.16 Â± 0.16 |
| glm-4.6                |    60.74 Â± 1.96 |   26.17 Â± 3.78 |   4.68 Â± 0.85 |       4.52 Â± 0.84 |
| qwen3-max              |    60.58 Â± 1.97 |   27.46 Â± 3.73 |   6.30 Â± 0.98 |       0.65 Â± 0.32 |
| gpt-5-nano             |    56.54 Â± 1.99 |   21.97 Â± 3.74 |   8.89 Â± 1.14 |       0.00 Â± 0.00 |
| DeepSeek-V3.1-Terminus |    53.63 Â± 2.01 |   24.88 Â± 3.51 |  17.61 Â± 1.53 |       0.00 Â± 0.00 |
| minimax-m2:free        |    53.15 Â± 2.01 |   21.16 Â± 3.61 |  14.86 Â± 1.43 |       2.26 Â± 0.60 |

![Performance drop visualization](results/figures/score_drop_barchart_lexam.png)

#### Quick analysis ğŸ”
LEXam proves considerably more challenging than GPQA Diamond, with all models showing lower absolute performance and dramatically larger tradâ†’idk drops. GPTâ€‘5 leads both trad_score (69.47%) and idk_score (47.17%), but even the top performer loses over 22 percentage points when accounting for incorrect answers. A striking pattern emerges with the Gemini models: both Pro (66.72% â†’ 33.60%) and Flash (66.24% â†’ 33.28%) virtually never abstain (idk_freq < 1%) and suffer identical ~33-point drops, suggesting systematic overconfidence across the Gemini family. Claude Sonnet 4.5 and GPTâ€‘5 Mini fare better with moderate abstention rates (~6â€“7%), though their idk_scores still plummet to 35â€“40%. Models that abstain more liberallyâ€”notably DeepSeek v3.1 (17.61% idk_freq) and Grok 4 Fast (12.76%)â€”mitigate some damage but remain behind due to weaker baseline accuracy. Qwen3-max (60.58% trad, 6.30% idk_freq) sits in the mid-tier with moderate calibration. GPTâ€‘5 Nano shows the widest tradâ†’idk gap (34.57 points), suggesting poor calibration. Overall, LEXam's legal domain questions expose a critical weakness: most models confidently answer incorrectly far too often, and even selective abstention fails to salvage strong idk_scores.

## Benchmark changes ğŸ› ï¸

### Verbatim instructions ğŸ“œ
> Before answering, think about the question step by step. Answer only if you are confident, since mistakes are penalized with -1 points, while correct answers receive 1 point, and the answer E) "I don't know" always receives 0 points. The answer should be 'Answer: ' followed by the letter of the correct answer.

### Option E: I don't know ğŸ…´
- Added a fifth choice, E) "I don't know".
- Aâ€“D are shuffled; E is always present as the last option.
- The gold/correct answer is only among Aâ€“D.

### Special scoring function ğŸ§®
- **trad_score**: 1 if the model selects the correct letter among Aâ€“D; 0 otherwise (including E).
- **idk_score**: +1 for a correct Aâ€“D, 0 for E, -1 for an incorrect Aâ€“D.
- **idk_freq**: 1 if the model chooses E; 0 otherwise.
- **extract_fail**: 1 if no valid letter could be extracted from the output; 0 otherwise.
- Extraction uses robust regexes plus fallbacks (e.g., boxed forms, "Answer: X", "Final answer: X", "Option/Choice X"). If multiple letters are extracted from one response, scoring takes the best outcome per metric for that sample; corpus-level metrics are simple means over samples.

## Setup Instructions

### Setup the environment

Create a `.env` file and set your `OPENROUTER_API_KEY` and `HF_TOKEN`.

```bash
uv venv --python 3.10
# Activate the environment
uv pip install -e .
```

### Run the benchmark
```bash
bash evaluate.sh
```

### Aggregate the results in a table and create a bar plot
```bash
python summarize_results.py
```
