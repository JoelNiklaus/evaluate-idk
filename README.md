# evaluate-idk
Evaluates to what extent LLMs signal correctly that they don't know the answer to a question

## Overview âœ¨
This repo evaluates how reliably LLMs say "I don't know" when they should, and how that calibration affects end-to-end utility. ðŸ¤–â“ It computes traditional accuracy, an IDK-aware score, the frequency of abstentions, and any extraction failures during parsing. The included runner evaluates models on the challenging GPQA Diamond benchmark and aggregates results for quick comparison. ðŸ§ªðŸ“Š

## Results ðŸ“Š
Latest aggregated results (as of September 26, 2025):

| Model                  | trad_score Â± se | idk_score Â± se | idk_freq Â± se | extract_fail Â± se |
| :--------------------- | --------------: | -------------: | ------------: | ----------------: |
| gemini-2.5-pro         |    83.84 Â± 2.62 |   67.68 Â± 5.25 |   0.00 Â± 0.00 |       2.02 Â± 1.00 |
| gpt-5                  |    82.83 Â± 2.69 |   68.69 Â± 5.03 |   3.03 Â± 1.22 |       3.03 Â± 1.22 |
| gpt-5-mini             |    79.29 Â± 2.89 |   60.10 Â± 5.63 |   1.52 Â± 0.87 |       1.01 Â± 0.71 |
| deepseek-v3.1-terminus |    71.21 Â± 3.23 |   47.47 Â± 6.06 |   5.05 Â± 1.56 |       0.00 Â± 0.00 |
| claude-sonnet-4        |    67.68 Â± 3.33 |   41.41 Â± 6.24 |   6.06 Â± 1.70 |       0.00 Â± 0.00 |
| gpt-5-nano             |    64.65 Â± 3.41 |   39.39 Â± 6.14 |  10.10 Â± 2.15 |       0.00 Â± 0.00 |
| gemini-2.5-flash       |    64.65 Â± 3.41 |   32.83 Â± 6.60 |   3.54 Â± 1.32 |       5.56 Â± 1.63 |
| gpt-4.1                |    63.13 Â± 3.44 |   27.78 Â± 6.79 |   1.52 Â± 0.87 |       0.00 Â± 0.00 |
| gpt-4.1-mini           |    61.62 Â± 3.46 |   27.27 Â± 6.70 |   4.04 Â± 1.40 |       0.00 Â± 0.00 |

![Performance drop visualization](results/figures/score_drop_barchart.png)

### Quick analysis ðŸ”Ž
IDK-aware performance broadly mirrors traditional accuracy, with one notable swap at the top: GPTâ€‘5 edges Gemini 2.5 Pro on idk_score even though Pro leads trad_score and Pro never abstains (idk_freq â‰ˆ 0). DeepSeek v3.1 and Claude Sonnet 4 use the E option relatively often, narrowing their tradâ†’idk gap but still trailing the leaders; GPTâ€‘4.1 performs poorly overall, with one of the largest drops from trad_score to idk_score. GPTâ€‘5 mini is a standout for its size, combining strong accuracy with solid idk_score. Apart from the GPTâ€‘5 vs Gemini Pro reversal, ordering by idk_score largely matches trad_score, though the size of the tradâ†’idk gap varies meaningfully across models. Smaller models tend to select E more (notably GPTâ€‘5 nano), which lifts idk_score enough to beat Gemini 2.5 Flash despite the same trad_score. Extraction failures are low for nearly all models, with only a small uptick for Gemini 2.5 Flash.

## Benchmark changes ðŸ› ï¸

### Verbatim instructions ðŸ“œ
> Before answering, think about the question step by step. Answer only if you are confident, since mistakes are penalized with -1 points, while correct answers receive 1 point, and the answer E) "I don't know" always receives 0 points. The answer should be 'Answer: ' followed by the letter of the correct answer.

### Option E: I don't know ðŸ…´
- Added a fifth choice, E) "I don't know". ([gpqa_diamond_idk.py:L70](gpqa_diamond_idk.py#L70))
- Aâ€“D are shuffled; E is always present as the last option. ([shuffle L57â€“L61](gpqa_diamond_idk.py#L57-L61), [E fixed L62â€“L71](gpqa_diamond_idk.py#L62-L71))
- The gold/correct answer is only among Aâ€“D. ([L78â€“L79](gpqa_diamond_idk.py#L78-L79))

### Special scoring function ðŸ§®
- **trad_score**: 1 if the model selects the correct letter among Aâ€“D; 0 otherwise (including E). ([doc L145â€“L149](gpqa_diamond_idk.py#L145-L149), [calc L233](gpqa_diamond_idk.py#L233), [agg L246](gpqa_diamond_idk.py#L246))
- **idk_score**: +1 for a correct Aâ€“D, 0 for E, -1 for an incorrect Aâ€“D. ([doc L146](gpqa_diamond_idk.py#L146), [rule L169â€“L176](gpqa_diamond_idk.py#L169-L176), [agg L247](gpqa_diamond_idk.py#L247))
- **idk_freq**: 1 if the model chooses E; 0 otherwise. ([doc L147](gpqa_diamond_idk.py#L147), [flag L232](gpqa_diamond_idk.py#L232), [agg L248](gpqa_diamond_idk.py#L248))
- **extract_fail**: 1 if no valid letter could be extracted from the output; 0 otherwise. ([doc L148â€“L149](gpqa_diamond_idk.py#L148-L149), [no-extract L229â€“L231](gpqa_diamond_idk.py#L229-L231), [agg L249](gpqa_diamond_idk.py#L249))
- Extraction uses robust regexes plus fallbacks (e.g., boxed forms, "Answer: X", "Final answer: X", "Option/Choice X"). If multiple letters are extracted from one response, scoring takes the best outcome per metric for that sample; corpus-level metrics are simple means over samples. ([fallback L91â€“L140](gpqa_diamond_idk.py#L91-L140), [regex L182â€“L184](gpqa_diamond_idk.py#L182-L184), [extract L187â€“L193](gpqa_diamond_idk.py#L187-L193), [dedupe L199â€“L201](gpqa_diamond_idk.py#L199-L201), [best-of L229â€“L235](gpqa_diamond_idk.py#L229-L235), [means L257](gpqa_diamond_idk.py#L257))

## Setup Instructions

### Setup the environment

Create a `.env` file and set your `OPENROUTER_API_KEY`.

```bash
uv venv --python 3.10
# Activate the environment
uv pip install -e .
```

### Run the benchmark
```bash
bash evaluate.sh
```

### Aggregate the results in a table
```bash
python summarize_results.py
```
